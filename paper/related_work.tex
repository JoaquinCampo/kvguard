% kvguard paper â€” Related Work
% Standalone file; \input{related_work.tex} from main paper
% Requires: No special packages

\section{Related Work}
\label{sec:related_work}

We organize related work along three axes: KV-cache compression methods (the systems kvguard wraps), recovery and adaptation mechanisms (the closest relatives), and generation quality monitoring (the signal sources kvguard builds on).


\subsection{KV-Cache Compression}

KV-cache compression methods reduce the memory footprint of autoregressive decoding by selectively retaining a subset of key-value pairs.
StreamingLLM~\cite{streamingllm2024} preserves a fixed window of recent tokens plus a small set of ``attention sink'' initial tokens, achieving constant memory at the cost of discarding intermediate context.
H2O~\cite{h2o2023} dynamically evicts tokens with the lowest cumulative attention score, adapting the cache to the model's evolving focus.
SnapKV~\cite{snapkv2024} clusters attention patterns during prefill to select a compact representative set per head.
PyramidKV~\cite{pyramidkv2024} allocates different cache budgets across layers based on attention entropy, assigning more cache to high-entropy (broadly attending) layers.

All of these methods are \emph{open-loop}: they make eviction decisions based on importance heuristics and never verify whether those decisions degraded downstream generation.
When compression is too aggressive, failures are silent---the model loops, fails to terminate, or produces corrupted reasoning---and standard average-accuracy evaluation hides these tails.
kvguard is designed to wrap any of these compressors, adding a feedback loop without modifying the compression algorithm itself.


\subsection{Recovery and Adaptation Mechanisms}

Several recent systems recognize that static compression can fail and attempt various forms of recovery or adaptation.
We discuss each in terms of its detection mechanism, response strategy, and architectural relationship to the underlying compressor.

\paragraph{ASR-KF-EGR}~\cite{asr_kf_egr2024} proposes a four-level entropy-guided recovery system (soft reset, window reset, full reset, rewalk regeneration) atop a reversible soft-freeze compression mechanism.
However, the recovery system is described only as future work: no entropy thresholds are defined, no triggering mechanism is implemented, and no evaluation is performed.
Furthermore, ASR-KF-EGR is itself a compressor (soft-freeze with sublinear scheduling), not a wrapper---it competes with StreamingLLM or SnapKV rather than making them safer.
The 5$\times$ overhead from CPU--GPU token transfers~\cite{asr_kf_egr2024} contrasts with kvguard's near-zero cost from logit-derived features.

\paragraph{RefreshKV}~\cite{refreshkv2024} alternates between partial-cache and full-cache attention steps, refreshing token selection at each full-attention step based on query similarity.
This provides genuine recovery (52\% of lost performance recovered on structured extraction tasks) but requires maintaining the complete KV cache throughout inference, yielding zero memory savings.
The refresh schedule is periodic or similarity-triggered, not failure-triggered: RefreshKV pays the refresh cost even when compression is performing correctly.

\paragraph{ERGO}~\cite{ergo2025} monitors rolling entropy delta between generation windows and triggers a full context reset when $\Delta H$ exceeds a calibrated threshold (85--90th percentile on $\sim$80 examples).
This achieves 56.6\% average performance gain and 35.3\% unreliability reduction, validating entropy as a detection signal.
However, the response is a sledgehammer: complete regeneration from scratch, discarding all generated text.
ERGO also operates at the generation level rather than the cache level---it does not adjust compression ratios or protect specific tokens.
kvguard builds on ERGO's validation of entropy signals while providing graduated responses and operating directly on the cache.

\paragraph{ThinKV}~\cite{thinkv2025} classifies reasoning into three modes (Reasoning, Transition, Execution) via attention sparsity metrics and applies different compression policies per mode (4-bit quantization for reasoning, 2-bit for execution).
This acknowledges that different reasoning phases tolerate different compression---an insight shared with kvguard's multi-mode controller.
However, ThinKV provides no mechanism to detect \emph{misclassification}: if a reasoning-critical token is classified as execution and aggressively compressed, there is no correction.
Classification also operates at 128-token granularity versus kvguard's per-token risk scoring.

\paragraph{Rethinking KV-Cache Compression}~\cite{rethinking_kv2025} uses a pre-generation complexity evaluator to route inputs to either compressed or full-cache inference.
This is an effective form of risk screening, but the routing decision is static---once made, the compression policy is fixed for the entire generation.
Within-generation dynamics (reasoning state evolving, compression failure emerging mid-sequence) cannot be detected or addressed.

\paragraph{DefensiveKV}~\cite{defensivekv2025} replaces snapshot-based importance scoring with max-over-history aggregation, preventing premature eviction of tokens whose importance is non-stationary.
Their finding that retained-token importance can drop to 0.34$\times$ its initial value provides strong empirical motivation for closed-loop monitoring: if importance scoring is fundamentally unstable, passive insurance cannot guarantee safety.
DefensiveKV is applied once at prefill and provides no runtime detection or recovery.

\paragraph{UNComp}~\cite{uncomp2024} uses matrix entropy to drive inter-layer and inter-head compression budgets, allocating more cache to low-entropy (focused-attention) heads.
Like DefensiveKV, this optimizes the initial compression policy but does not monitor generation quality at runtime.

\paragraph{Summary.}
Table~\ref{tab:related_comparison} positions kvguard against these systems.
The key differentiators are:
(i) kvguard is a \emph{wrapper} that makes any compressor safer, not a competing compressor;
(ii) it uses a \emph{trained} hazard predictor on a 42-dimensional feature vector rather than a single-signal threshold;
(iii) it provides \emph{graduated} response (alert $\to$ safe $\to$ recovery) rather than all-or-nothing reset;
and (iv) it evaluates on a \emph{catastrophe-specific} metric (CFR) rather than average accuracy.

\begin{table*}[t]
\centering
\small
\caption{Comparison of kvguard with related systems along five key dimensions. \textbf{Bold} indicates a unique capability.}
\label{tab:related_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{System} & \textbf{Wrapper?} & \textbf{Runtime Detection} & \textbf{Recovery} & \textbf{Memory Savings} & \textbf{Overhead} \\
\midrule
ASR-KF-EGR & No & Proposed (unimpl.) & Proposed (unimpl.) & 55--67\% & 5$\times$ \\
RefreshKV & No & None & Full-cache refresh & None & $\sim$0 \\
ERGO & Partial & Entropy threshold & Full reset & N/A & Reset cost \\
ThinKV & No & None & None & Yes & $\sim$0 \\
Rethinking KV & Partial & Pre-generation only & Route to full cache & Varies & Evaluator \\
DefensiveKV & No & None & None & Yes & $\sim$0 \\
UNComp & No & None & None & Yes & $\sim$0 \\
\midrule
\textbf{kvguard} & \textbf{Yes} & \textbf{Trained predictor} & \textbf{Graduated} & \textbf{Preserved} & \textbf{$\sim$0} \\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Generation Quality Monitoring}

kvguard's hazard predictor draws on three lines of work that provide the detection signals.

\paragraph{Logit-based quality prediction.}
The HALT framework~\cite{halt2024} demonstrates that top-$k$ log-probabilities from a model's own output distribution predict generation quality with minimal computational overhead.
These features are a byproduct of the softmax already computed during decoding, making them essentially free.
kvguard uses 25 HALT-derived features (top-20 log-probabilities, max, sum, mean, entropy, and varentropy of the logit distribution) as the core of its 42-dimensional feature vector.

\paragraph{Thinking tokens and reasoning dynamics.}
Recent work on reasoning dynamics~\cite{reasoning_dynamics2025} identifies a small vocabulary of ``thinking tokens'' (e.g., ``so'', ``therefore'', ``wait'', ``hmm'') comprising 1--5\% of generated tokens whose suppression disproportionately degrades chain-of-thought reasoning.
kvguard uses a thinking-token flag as a feature and, in alert mode, protects these tokens from compression.

\paragraph{Receiver heads and attention anchors.}
Thought anchors~\cite{thought_anchors2025} identifies attention heads that consistently concentrate on a few ``anchor'' tokens carrying disproportionate information.
kvguard's recovery mechanism uses receiver-head identification to perform selective KV recomputation for anchor tokens, enabling surgical recovery rather than full-cache restoration.

Together, these signals enable kvguard's near-zero-cost monitoring: the features are extracted from quantities already computed during standard autoregressive decoding, requiring no additional forward passes or attention recomputation.
