% kvguard paper â€” Method section
% Standalone file; \input{method.tex} from main paper
% Requires: booktabs, amsmath, amssymb, algorithm2e or algorithmic, xcolor

\section{Method}
\label{sec:method}

We present kvguard, a closed-loop controller that wraps any existing KV-cache compressor and dynamically adjusts compression aggressiveness based on runtime signals.
The key insight is that catastrophic compression failures---looping, non-termination, reasoning corruption---produce detectable precursor signals in the model's own logit distribution, and these signals can be monitored at near-zero cost during decoding.
The system has four components: a trace collection pipeline (\S\ref{sec:method:traces}), a per-token feature extractor (\S\ref{sec:method:features}), a trained hazard predictor (\S\ref{sec:method:predictor}), and a state-machine controller (\S\ref{sec:method:controller}).

%----------------------------------------------------------------------
\subsection{Problem Formulation}
\label{sec:method:problem}

Let $\mathcal{C}$ be a KV-cache compressor (e.g., StreamingLLM, SnapKV, H2O) parameterized by a compression ratio $r \in [0,1]$ denoting the fraction of KV entries removed.
At each decoding step $t$, the model produces a token $x_t$ and a logit vector $\ell_t \in \mathbb{R}^{|V|}$.
We define a \emph{catastrophic failure} as any of: (i)~looping---a 20-token window repeated $\geq$3 times; (ii)~non-termination---generation reaches \texttt{max\_new\_tokens} without producing EOS; or (iii)~answer failure---the final answer is incorrect.
The \emph{Catastrophic Failure Rate} (CFR) at ratio $r$ is the fraction of prompts exhibiting any catastrophe.

The controller's goal is to minimize CFR while preserving compression savings.
Formally, we seek a policy $\pi$ that observes per-token signals $s_t$ and outputs a (possibly adjusted) compression ratio $r_t$, such that CFR under $\pi$ is substantially lower than CFR under the static policy $r_t = r$ for all $t$.

%----------------------------------------------------------------------
\subsection{Trace Collection}
\label{sec:method:traces}

We generate a behavior dataset by running Qwen2.5-3B-Instruct on 50 GSM8K prompts (3-shot chain-of-thought) under each combination of compressor and ratio.
Three compressors from the \texttt{kvpress} library are used---StreamingLLM, SnapKV, and ObservedAttention---at five compression ratios (0.25, 0.50, 0.625, 0.75, 0.875), plus an uncompressed baseline, yielding 16 configurations and 800 total traces.

For each trace, we record:
\begin{itemize}
\item The full generated token sequence under greedy decoding (\texttt{max\_new\_tokens}${}=512$).
\item Per-token logit-derived signals (entropy, top-$k$ probabilities, rank of chosen token, top-20 log-probabilities); see \S\ref{sec:method:features}.
\item Per-token repetition counts: a sliding 20-token window tracks how many times the current window has appeared so far.
\item Catastrophe labels and onset positions.
      For \emph{looping}, the onset is the token index where the second occurrence of a repeated window begins.
      For \emph{non-termination}, the true onset (the last token) is uninformative; we use a proxy onset at $\lfloor 0.75 \cdot \texttt{max\_new\_tokens} \rfloor = 384$.
      \emph{Wrong-answer} failures have no per-token onset and are excluded from the hazard prediction target.
\end{itemize}

Each compressor--ratio configuration runs in an isolated process with a 300-second per-prompt timeout and per-prompt checkpointing for fault tolerance.

%----------------------------------------------------------------------
\subsection{Per-Token Feature Extraction}
\label{sec:method:features}

At each decoding step $t$, we extract a 42-dimensional feature vector $\mathbf{f}_t$ from the logit distribution $\ell_t$ and decoding history.
The features are designed to be \emph{zero additional cost}: they are computed from quantities already available during standard autoregressive generation.

\paragraph{Base features (30 dimensions).}
From the softmax distribution $p_t = \mathrm{softmax}(\ell_t)$:
\begin{itemize}
\item \textbf{Entropy} $H_t = -\sum_v p_t(v) \log p_t(v)$: overall uncertainty.
\item \textbf{Top-1 and top-5 probabilities}: confidence in the greedy choice and the top-5 tokens.
\item \textbf{Rank of chosen token}: always 0 under greedy decoding; informative under sampling.
\item \textbf{Top-20 log-probabilities} ($\log p_t(v)$ for the 20 highest-probability tokens): the HALT feature set~\cite{halt2024}, which captures the shape of the logit distribution beyond the argmax.
\item \textbf{$h_{\text{alts}}$}: average log-probability of the top-20 alternatives, measuring how concentrated probability mass is.
\item \textbf{$\Delta H_t = H_t - H_{t-1}$}: entropy change between consecutive tokens, with a validity flag for $t=0$.
\item \textbf{Repetition count}: number of times the current 20-token window has appeared so far.
\item \textbf{Thinking-token flag}: binary indicator for tokens in a 31-word reasoning vocabulary (e.g., ``so'', ``wait'', ``therefore'', ``hmm''), drawn from work showing that suppressing these 1--5\% of tokens disproportionately degrades reasoning~\cite{reasoning_dynamics2025}.
\end{itemize}

\paragraph{Rolling features (11 dimensions).}
To capture temporal dynamics without future leakage, we compute \emph{causal} rolling statistics over a lookback window of $w=8$ tokens:
\begin{itemize}
\item Rolling mean and standard deviation of entropy, top-1 probability, $h_{\text{alts}}$, and $\Delta H$ (8 features).
\item Rolling sum of repetition count (1 feature).
\end{itemize}
For positions $t < w$, the window grows from 1 to $w-1$ tokens, ensuring no padding artifacts.

\paragraph{Context features (2 dimensions).}
\begin{itemize}
\item \textbf{Token position}: $t / (T-1)$, normalized to $[0,1]$. Captures position-dependent failure risk (e.g., non-termination signals strengthen toward the end).
\item \textbf{Compression ratio}: the static ratio $r$ applied to this trace. This is known at inference time and lets the predictor learn compressor-specific risk profiles.
\end{itemize}

All features are causal (depend only on tokens $\leq t$), zero additional cost (derived from quantities already computed during decoding), and compressor-agnostic (no access to internal compressor state).

%----------------------------------------------------------------------
\subsection{Hazard Prediction}
\label{sec:method:predictor}

We train a binary classifier to predict, at each token $t$, whether a catastrophe will occur within the next $H$ tokens.
This framing---predicting a hazard window rather than instantaneous failure---gives the controller lead time to intervene.

\paragraph{Label construction.}
For a trace with earliest catastrophe onset at token $t^*$, the hazard label is:
\begin{equation}
y_t =
\begin{cases}
1 & \text{if } t \geq t^* - H \\
0 & \text{otherwise}
\end{cases}
\label{eq:hazard_label}
\end{equation}
We use horizon $H=32$.
Only looping and non-termination receive per-token labels; wrong-answer failures are excluded because they produce no detectable per-token onset signal.
When multiple catastrophe types co-occur, the earliest onset determines $t^*$.

\paragraph{Model.}
We use XGBoost with 200 trees, max depth 6, learning rate 0.1, and subsampling at 0.8.
Class imbalance (13\% positive rate) is handled via \texttt{scale\_pos\_weight}.
The choice of gradient-boosted trees over neural alternatives is deliberate: XGBoost trains in seconds on 263K tokens, requires no GPU, produces calibrated probabilities, and provides interpretable feature importance---properties that matter for a safety-critical runtime component.

\paragraph{Evaluation protocol.}
We split at the \emph{trace level} (not token level) to prevent data leakage: a trace cannot appear in both train and validation.
The split is stratified by catastrophe presence to ensure both classes appear in each partition.
Cross-compressor generalization is evaluated via \emph{leave-one-compressor-out} cross-validation: for each compressor, we train on traces from the other two and evaluate on the held-out compressor.
This tests whether the predictor transfers to an unseen compression method---a prerequisite for the compressor-agnostic claim.

%----------------------------------------------------------------------
\subsection{Controller Design}
\label{sec:method:controller}

The controller is a per-token state machine inspired by TCP congestion control: it begins with aggressive compression, monitors for signs of trouble, and backs off when hazard is detected.
Unlike TCP, which adjusts a single parameter (window size), the controller adjusts both the compression ratio and token-protection policy.

\paragraph{Modes.}
The controller operates in four modes, ordered by severity:

\smallskip
\noindent
\begin{tabular}{llp{5.5cm}}
\toprule
\textbf{Mode} & \textbf{Ratio} & \textbf{Action} \\
\midrule
\textsc{Normal} & $r_{\text{base}}$ & Standard compression \\
\textsc{Alert} & $r_{\text{base}}$ & Protect thinking tokens from eviction \\
\textsc{Safe} & $r_{\text{safe}}$ & Relax compression ratio \\
\textsc{Recovery} & 0 & Selective KV recomputation \\
\bottomrule
\end{tabular}
\smallskip

\noindent
In the evaluated configuration, $r_{\text{base}} = 0.875$ and $r_{\text{safe}} = 0$ (fall back to no compression).

\paragraph{Escalation and de-escalation.}
Let $\hat{p}_t$ denote the hazard predictor's output probability at token $t$.
Escalation requires $k$ \emph{consecutive} tokens above a threshold (hysteresis):

\smallskip
\noindent
\begin{tabular}{lll}
\textsc{Normal} $\to$ \textsc{Alert}: & $\hat{p}_t > \tau_{\text{low}}$ & for $k$ consecutive tokens \\
\textsc{Alert} $\to$ \textsc{Safe}: & $\hat{p}_t > \tau_{\text{high}}$ & for $k$ consecutive tokens \\
\textsc{Safe} $\to$ \textsc{Recovery}: & rep\_count $\geq 3$ & (immediate) \\
\end{tabular}
\smallskip

\noindent
De-escalation requires $j$ consecutive tokens \emph{below} the threshold, with $j > k$ to implement conservative hysteresis---it is easier to raise an alarm than to lower it.

\paragraph{Hysteresis as cumulative detection.}
The consecutive-token requirement is the key design choice.
A single high-risk token may be noise; $k$ consecutive high-risk tokens indicate a genuine trend.
This converts the per-token predictor (which may have modest recall) into a reliable per-trace trigger.
In our evaluation, the predictor has only 69\% token-level recall at 5\% FPR, yet the controller with $k=8$ achieves 100\% trace-level detection---because every catastrophe trace produces \emph{some} run of elevated hazard probabilities.

\paragraph{Operating points.}
The controller exposes three tunable parameters: $\tau_{\text{low}}$, $\tau_{\text{high}}$, and $k$.
We evaluate three operating points spanning the prevention--selectivity tradeoff:

\smallskip
\noindent
\begin{tabular}{lccc}
& $\tau_{\text{low}}/\tau_{\text{high}}$ & $k$ & Character \\
\midrule
Conservative & 0.7 / 0.8 & 5 & Low FP, moderate prevention \\
Balanced & 0.3 / 0.7 & 8 & Best prevention-to-FP ratio \\
Aggressive & 0.3 / 0.6 & 5 & High prevention, higher FP \\
\end{tabular}

%----------------------------------------------------------------------
\subsection{Anchor-Aware Token Protection}
\label{sec:method:anchors}

When the controller enters \textsc{Alert} or \textsc{Safe} mode, it must decide \emph{which} KV entries to protect from eviction.
We identify critical positions using \emph{receiver heads}---attention heads that consistently concentrate attention on a few ``anchor'' tokens carrying disproportionate information~\cite{thought_anchors2025}.

\paragraph{Calibration.}
Given attention tensors from calibration traces (shape: layers $\times$ heads $\times$ seq $\times$ seq), we compute a \emph{vertical attention score} for each key position $j$ through each head:
\begin{equation}
v_j = \frac{1}{|\{i : i \geq j + d\}|} \sum_{i \geq j + d} A_{i,j}
\end{equation}
where $A_{i,j}$ is the attention weight from query $i$ to key $j$ and $d=32$ is a minimum distance that filters out local attention patterns.
Heads with high excess kurtosis in their vertical attention scores---i.e., ``spiky'' heads that attend strongly to a few positions---are selected as the top-$K$ receiver heads.

\paragraph{Runtime anchor identification.}
During generation, we compute vertical attention scores through only the calibrated receiver heads, average across heads, and threshold at the 90th percentile.
Positions above this threshold are marked as \emph{anchors} and protected from eviction in \textsc{Alert}/\textsc{Safe} mode.
In \textsc{Recovery} mode, anchor KV entries are candidates for selective recomputation.

%----------------------------------------------------------------------
\subsection{Evaluation Framework}
\label{sec:method:evaluation}

We evaluate the controller via \emph{offline simulation} on the 800 collected traces.
For each trace at ratio $r$ where the controller triggers \textsc{Safe} mode (transitions before catastrophe onset), we check whether the same prompt at $r_{\text{safe}} = 0$ (no compression) avoids catastrophe.
If so, the catastrophe is counted as \emph{prevented}.

This simulation makes a simplifying assumption: that switching to $r_{\text{safe}}$ mid-generation recovers the same behavior as never compressing.
This may not hold if early compressed tokens have already corrupted the reasoning state.
The simulation therefore provides an \emph{upper bound} on live controller performance.

We report:
\begin{itemize}
\item \textbf{CFR reduction}: $(\text{CFR}_{\text{baseline}} - \text{CFR}_{\text{controlled}}) / \text{CFR}_{\text{baseline}}$, the relative decrease in catastrophic failures.
\item \textbf{False positive rate}: fraction of non-catastrophe traces where the controller triggers unnecessarily.
\item \textbf{Per-compressor breakdown}: verifying the controller's benefit generalizes across compression methods.
\end{itemize}
