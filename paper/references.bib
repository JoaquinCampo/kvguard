% kvguard paper â€” Bibliography
% 15 entries matching all \cite{} keys across sections

% --- KV-Cache Compression ---

@inproceedings{streamingllm2024,
  title     = {Efficient Streaming Language Models with Attention Sinks},
  author    = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle = {International Conference on Learning Representations},
  year      = {2024},
  url       = {https://arxiv.org/abs/2309.17453}
}

@inproceedings{snapkv2024,
  title     = {{SnapKV}: {LLM} Knows What You are Looking for Before Generation},
  author    = {Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {37},
  year      = {2024},
  url       = {https://arxiv.org/abs/2404.14469}
}

@inproceedings{h2o2023,
  title     = {{H$_2$O}: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author    = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang and Chen, Beidi},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {36},
  year      = {2023},
  url       = {https://arxiv.org/abs/2306.14048}
}

@article{pyramidkv2024,
  title   = {{PyramidKV}: Dynamic {KV} Cache Compression based on Pyramidal Information Funneling},
  author  = {Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Li, Yucheng and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Hu, Junjie and Xiao, Wen},
  journal = {arXiv preprint arXiv:2406.02069},
  year    = {2024},
  url     = {https://arxiv.org/abs/2406.02069}
}

% --- Recovery and Adaptation ---

@article{asr_kf_egr2024,
  title   = {Adaptive Soft Rolling {KV} Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient {LLM} Inference},
  author  = {Metinov, Adilet and Kudakeeva, Gulida M. and Nursultan, Bolotbek uulu and Kabaeva, Gulnara D.},
  journal = {arXiv preprint arXiv:2512.11221},
  year    = {2024},
  url     = {https://arxiv.org/abs/2512.11221}
}

@article{refreshkv2024,
  title   = {{RefreshKV}: Updating Small {KV} Cache During Long-form Generation},
  author  = {Xu, Fangyuan and Goyal, Tanya and Choi, Eunsol},
  journal = {arXiv preprint arXiv:2411.05787},
  year    = {2024},
  url     = {https://arxiv.org/abs/2411.05787}
}

@inproceedings{ergo2025,
  title     = {{ERGO}: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models},
  author    = {Khalid, Haziq Mohammad and Jeyaganthan, Athikash and Do, Timothy and Fu, Yicheng and O'Brien, Sean and Sharma, Vasu and Zhu, Kevin},
  booktitle = {Proceedings of the 2nd Workshop on Uncertainty Aware {NLP} ({UncertaiNLP})},
  pages     = {273--286},
  year      = {2025},
  publisher = {Association for Computational Linguistics},
  url       = {https://arxiv.org/abs/2510.14077}
}

@article{thinkv2025,
  title   = {{ThinKV}: Thought-Adaptive {KV} Cache Compression for Efficient Reasoning Models},
  author  = {Ramachandran, Akshat and Neseem, Marina and Sakr, Charbel and Venkatesan, Rangharajan and Khailany, Brucek and Krishna, Tushar},
  journal = {arXiv preprint arXiv:2510.01290},
  year    = {2025},
  url     = {https://arxiv.org/abs/2510.01290}
}

@article{rethinking_kv2025,
  title   = {Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving},
  author  = {Gao, Wei and Zhou, Xinyu and Sun, Peng and Zhang, Tianwei and Wen, Yonggang},
  journal = {arXiv preprint arXiv:2503.24000},
  year    = {2025},
  url     = {https://arxiv.org/abs/2503.24000}
}

@article{defensivekv2025,
  title   = {Taming the Fragility of {KV} Cache Eviction in {LLM} Inference},
  author  = {Feng, Yuan and Guo, Haoyu and Lv, JunLin and Zhou, S. Kevin and Xie, Xike},
  journal = {arXiv preprint arXiv:2510.13334},
  year    = {2025},
  url     = {https://arxiv.org/abs/2510.13334}
}

@inproceedings{uncomp2024,
  title     = {{UNComp}: Can Matrix Entropy Uncover Sparsity? {A} Compressor Design from an Uncertainty-Aware Perspective},
  author    = {Xiong, Jing and Shen, Jianghan and Ye, Fanghua and Tao, Chaofan and Wan, Zhongwei and Lu, Jianqiao and Wu, Xun and Zheng, Chuanyang and Guo, Zhijiang and Yang, Min and Kong, Lingpeng and Wong, Ngai},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  year      = {2025},
  url       = {https://arxiv.org/abs/2410.03090}
}

% --- Generation Quality Monitoring ---

@article{halt2024,
  title   = {{HALT}: Hallucination Assessment via Log-probs as Time Series},
  author  = {Shapiro, Ahmad and Taneja, Karan and Goel, Ashok},
  journal = {arXiv preprint arXiv:2602.02888},
  year    = {2026},
  url     = {https://arxiv.org/abs/2602.02888}
}

@article{reasoning_dynamics2025,
  title   = {Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in {LLM} Reasoning},
  author  = {Qian, Chen and Liu, Dongrui and Wen, Haochen and Bai, Zhen and Liu, Yong and Shao, Jing},
  journal = {arXiv preprint arXiv:2506.02867},
  year    = {2025},
  url     = {https://arxiv.org/abs/2506.02867}
}

@article{thought_anchors2025,
  title   = {Thought Anchors: Which {LLM} Reasoning Steps Matter?},
  author  = {Bogdan, Paul C. and Macar, Uzay and Nanda, Neel and Conmy, Arthur},
  journal = {arXiv preprint arXiv:2506.19143},
  year    = {2025},
  url     = {https://arxiv.org/abs/2506.19143}
}

% --- Classic Networking ---

@inproceedings{tcp_reno,
  title     = {Congestion Avoidance and Control},
  author    = {Jacobson, Van and Karels, Michael J.},
  booktitle = {Proceedings of the {ACM} {SIGCOMM} Conference on Communications Architectures and Protocols},
  pages     = {314--329},
  year      = {1988},
  publisher = {ACM},
  doi       = {10.1145/52324.52356}
}
