% kvguard paper â€” Discussion and Conclusion
% Standalone file; \input{discussion.tex} from main paper
% Requires: No special packages

\section{Discussion}
\label{sec:discussion}

\subsection{Why Cumulative Detection Works}
\label{sec:discussion:cumulative}

The most important finding is not the controller's aggregate CFR reduction but \emph{how} it achieves it.
The per-token hazard predictor has modest recall (69\% at 5\% FPR overall, 41\% for StreamingLLM), yet the controller achieves 100\% trace-level detection and 63.9\% CFR reduction (excluding immediate-onset cases).
The gap between weak per-token performance and strong per-trace performance is entirely due to hysteresis: requiring $k=8$ consecutive high-risk tokens before escalation.

This mechanism mirrors a well-known principle in signal processing: noisy detectors become reliable when integrated over time.
A single token above threshold is often noise.
Eight consecutive tokens above threshold is a trend.
Catastrophe traces produce extended runs of elevated hazard probability---not because every individual token is high-confidence, but because the underlying failure mode (looping, entropy drift, repetition) creates a persistent signal that the predictor partially captures at each step.
The state machine accumulates this partial signal into a reliable trigger.

This has a practical implication: the hazard predictor need not be a near-perfect per-token classifier.
It needs only to produce \emph{some} elevation in risk score during pre-catastrophe windows.
This relaxes the accuracy requirement on the ML component and makes the system robust to moderate predictor degradation---a desirable property for safety-critical applications.

\subsection{The Closed-Loop Paradigm}
\label{sec:discussion:paradigm}

All KV-cache compressors to date operate in open loop: they score tokens, evict, and proceed without feedback.
This is analogous to early network congestion control, where senders transmitted at a fixed rate without monitoring whether packets were arriving.
TCP congestion control~\cite{tcp_reno} solved that problem by closing the loop---adjusting the sending rate based on observed loss---and the analogy transfers directly to cache compression.

The wrapper architecture makes this concrete: kvguard does not compete with compressors, it \emph{makes them safer}.
Every advance in compression methods (better importance scoring, smarter eviction policies, new quantization schemes) automatically inherits the safety layer.
This orthogonality is not incidental; it is the core architectural choice.
A safety mechanism that is entangled with a specific compressor must be re-engineered whenever the compressor changes.
A wrapper that monitors output quality and adjusts a single knob (compression ratio) remains valid regardless of the underlying compression strategy.

\subsection{CFR as a Complementary Metric}
\label{sec:discussion:cfr}

Standard KV-cache compression evaluation reports average accuracy or perplexity under compression.
This hides catastrophic failures: in our experiments, ObservedAttention at 0.625 compression reports 36\% accuracy---but this single number conceals that 64\% of prompts fail, many through looping or non-termination rather than graceful quality degradation.
The Catastrophic Failure Rate separates \emph{how often} compression causes complete failure from \emph{how much} it degrades average quality, and the two tell different stories.

We do not argue that CFR should replace average accuracy; both are needed.
But for safety-critical deployments---medical reasoning, financial analysis, agentic systems where a single looping generation can block a pipeline---CFR is the metric that matters.
A compressor with 90\% average accuracy and 2\% CFR is safer than one with 92\% average accuracy and 8\% CFR, even though standard benchmarks would prefer the latter.

\subsection{Limitations}
\label{sec:discussion:limitations}

\paragraph{Immediate-onset failures remain undetectable.}
SnapKV at 0.875 compression produces looping within the first 5 tokens---before any online monitor can accumulate sufficient signal.
This is an inherent limitation of reactive monitoring: some compression configurations are so aggressive that failure is instantaneous.
The practical mitigation is pre-generation screening~\cite{rethinking_kv2025}: route known-aggressive configurations to full-cache inference and apply the controller only at moderate ratios where it can be effective.
The two approaches are complementary.

\paragraph{Quiet failures produce weak signals.}
The gap between our 43.2\% overall CFR reduction and the 61.3\% always-safe ceiling is primarily due to wrong-answer failures that produce no detectable signal: no looping, no entropy spike, no repetition.
These ``quiet'' failures corrupt the reasoning chain without overt behavioral markers.
Detecting them likely requires deeper signals---attention patterns, internal representation drift, or semantic coherence monitoring---that go beyond the zero-cost logit features used in this work.

\paragraph{Offline simulation overstates live performance.}
Our evaluation assumes that switching to no compression mid-generation recovers the same behavior as never compressing.
In practice, tokens generated under aggressive compression may have already corrupted the reasoning state, making recovery impossible regardless of subsequent compression policy.
The reported CFR reduction is therefore an upper bound.
Live token-by-token evaluation is needed to quantify the gap between simulation and reality.

\paragraph{Single model and task.}
All experiments use Qwen2.5-3B-Instruct on GSM8K.
While leave-one-compressor-out cross-validation validates signal transferability across compression methods, generalization to other model families (Llama, Phi), scales (7B, 70B), and tasks (coding, document QA, agentic workflows) remains unvalidated.
Larger models may produce richer logit signals, improving predictor performance; alternatively, they may fail in qualitatively different ways that the current feature set does not capture.

\paragraph{Fixed safe ratio.}
The controller currently falls back to $r_{\text{safe}} = 0$ (no compression), which maximizes recovery probability but sacrifices all memory savings during intervention.
A more graduated approach---reducing compression from 0.75 to 0.50 rather than to 0---might preserve partial savings while still preventing catastrophe.
This requires understanding the relationship between compression reduction and recovery probability, which our current dataset cannot quantify.

\subsection{Future Work}
\label{sec:discussion:future}

\paragraph{Live token-by-token integration.}
The most immediate priority is validating the controller in a live autoregressive loop where compression ratio changes take effect at the next decoding step.
This will reveal whether mid-generation compression switching recovers generation quality as the offline simulation assumes, and will quantify the true CFR reduction in deployment conditions.

\paragraph{Quiet failure detection.}
The current feature set is optimized for loud failures (looping, non-termination) that produce strong behavioral signals.
Detecting quiet wrong-answer failures---where the model follows a plausible but incorrect reasoning path---likely requires attention-derived features (lookback ratio, attention entropy over receiver heads) or semantic coherence signals.
These were deferred from the current scope but represent the most impactful extension for closing the gap to the always-safe ceiling.

\paragraph{Multi-model and multi-task evaluation.}
Extending to additional model families, scales, and reasoning tasks would strengthen generalization claims.
Of particular interest is whether the feature importance ranking (compression ratio $>$ repetition $>$ position $>$ HALT alternatives) is model-specific or universal.
If universal, a single predictor could serve as a general-purpose safety layer.

\paragraph{Adaptive safe ratio.}
Rather than a binary switch between base compression and no compression, an adaptive controller could search for the minimum compression relaxation needed to prevent catastrophe, preserving partial memory savings during intervention periods.

%----------------------------------------------------------------------

\section{Conclusion}
\label{sec:conclusion}

KV-cache compression enables long-context inference under memory constraints, but existing compressors operate in open loop: they evict tokens without feedback and never verify whether eviction degraded generation quality.
We have shown that this open-loop approach produces catastrophic failures---looping, non-termination, and reasoning corruption---that standard average-accuracy metrics hide, affecting 44\% of traces at moderate-to-aggressive compression ratios across three compressors.

We presented kvguard, a closed-loop controller inspired by TCP congestion control that wraps any existing KV-cache compressor.
The system extracts a 42-dimensional per-token feature vector from the model's own logit distribution at near-zero cost, trains a hazard predictor that achieves 0.945 AUROC with 34-token mean lead time before catastrophe onset, and uses a hysteresis-based state machine to dynamically relax compression when danger is detected.

The controller reduces the Catastrophic Failure Rate by 43.2\% overall (63.9\% excluding immediate-onset cases) with only 6.9\% false positive rate, exceeding 50\% CFR reduction on all three compressors.
Systematic ablations confirm that each component contributes independently: the trained predictor adds 42.6 percentage points over random, and hysteresis reduces false positives by 53 percentage points at a 17 percentage point prevention cost.

The key mechanism is cumulative detection: requiring $k$ consecutive high-risk tokens converts a modest per-token classifier into a reliable per-trace trigger.
This principle---that a weak detector integrated over time becomes a strong one---suggests that closing the loop on KV-cache compression does not require perfect per-token prediction, only persistent signal during pre-catastrophe windows.
We believe this closed-loop paradigm will become a standard component of KV-cache management as compression is deployed in safety-critical applications.
