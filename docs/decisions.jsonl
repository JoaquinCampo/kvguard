{"date": "2026-02-09", "title": "Use kvpress as primary compression framework", "context": "Need existing compressor implementations rather than reimplementing from literature", "options": [{"name": "kvpress", "detail": "32 methods, NVIDIA maintained, HF Transformers native"}, {"name": "KVCache-Factory", "detail": "R-KV support, Flash Attention, but narrower scope"}, {"name": "Cold Compress", "detail": "Answer.AI, torch.compile, but opinionated stack"}, {"name": "from-scratch", "detail": "Full control, months of work"}], "chosen": "kvpress", "rationale": "Unified API with 32 swappable presses — our controller wraps a Press, which is literally the wrapper architecture from the SOTA synthesis. Actively maintained by NVIDIA.", "references": ["https://github.com/NVIDIA/kvpress", "docs/plans/2026-02-09-implementation-plan.md"]}
{"date": "2026-02-17", "title": "Start with Qwen2.5-3B-Instruct for prototyping", "context": "Need a model to validate the experiment pipeline before scaling to 7B+", "options": [{"name": "Qwen2.5-3B-Instruct", "detail": "No license gate, fits MPS, fast iteration"}, {"name": "Llama-3-8B-Instruct", "detail": "Popular baseline, but gated and needs more memory"}, {"name": "Qwen2.5-7B-Instruct", "detail": "Better quality, but slower on MPS"}, {"name": "Mistral-7B-v0.3", "detail": "Strong model, but less kvpress testing coverage"}], "chosen": "Qwen2.5-3B-Instruct", "rationale": "No license friction, runs on Apple Silicon MPS in float16, kvpress compatible. Good enough to validate pipeline — real paper experiments will use 7B+ on the 5090.", "references": ["src/kvguard/config.py"]}
{"date": "2026-02-17", "title": "Use GSM8K as first reasoning benchmark", "context": "Need a task with multi-step CoT reasoning and trivially parseable answers", "options": [{"name": "GSM8K", "detail": "Grade-school math, canonical, simple answer extraction"}, {"name": "MATH", "detail": "Harder problems, longer traces, but complex parsing"}, {"name": "AIME", "detail": "Competition math, very long reasoning"}, {"name": "LongBench", "detail": "Long-context tasks, but not reasoning-focused"}], "chosen": "GSM8K", "rationale": "Multi-step CoT reasoning with numeric answers that are trivial to extract and verify. HuggingFace datasets support. Standard benchmark that reviewers expect.", "references": ["src/kvguard/prompts.py"]}
{"date": "2026-02-17", "title": "Use compression_ratio as budget normalization", "context": "Need a consistent way to compare compression levels across methods", "options": [{"name": "compression_ratio (kvpress)", "detail": "Fraction of cache removed, matches library API"}, {"name": "retained tokens per layer", "detail": "More interpretable, but requires manual calculation"}, {"name": "total KV bytes", "detail": "Accounts for precision differences, but complex"}], "chosen": "compression_ratio (kvpress)", "rationale": "Directly maps to the kvpress API parameter. Simple and consistent across all press types.", "references": ["src/kvguard/config.py"]}
{"date": "2026-02-17", "title": "Support both #### and boxed answer formats", "context": "Qwen2.5-3B-Instruct outputs LaTeX boxed format despite few-shot examples using ####", "options": [{"name": "#### only", "detail": "Standard GSM8K format"}, {"name": "\\boxed{} only", "detail": "What instruction-tuned models actually produce"}, {"name": "Both", "detail": "Parse either format"}], "chosen": "Both", "rationale": "Instruction-tuned models prefer \\boxed{} regardless of few-shot examples. Supporting both avoids systematic false negatives in accuracy measurement.", "references": ["src/kvguard/detectors.py"]}
{"date": "2026-02-17", "title": "Use output_scores for signal extraction instead of manual generation loop", "context": "Need per-token logits during generation for entropy and top-k computation", "options": [{"name": "output_scores=True", "detail": "HF generate() returns per-step logits natively"}, {"name": "Manual loop", "detail": "Full control, but reimplements generation logic"}], "chosen": "output_scores=True", "rationale": "HF model.generate() with return_dict_in_generate=True gives per-step logits without reimplementing the generation loop. Works correctly with kvpress context manager.", "references": ["src/kvguard/experiment.py", "src/kvguard/signals.py"]}
